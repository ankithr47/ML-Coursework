{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cbbf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brain_seen, all_image_seen, all_text_seen, all_label_seen = [], [], [], []\n",
    "all_brain_unseen, all_image_unseen, all_text_unseen, all_label_unseen = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5117a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_brain_samples= 49620 , seen_brain_features= 561\n",
      "seen_image_samples= 49620 , seen_image_features= 100\n",
      "seen_text_samples= 49620 , seen_text_features= 512\n",
      "seen_label= torch.Size([49620, 1])\n",
      "unseen_brain_samples= 48000 , unseen_brain_features= 561\n",
      "unseen_image_samples= 48000 , unseen_image_features= 100\n",
      "unseen_text_samples= 48000 , unseen_text_features= 512\n",
      "unseen_label= torch.Size([48000, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data\n",
    "import mmbra\n",
    "import mmbracategories\n",
    "import torch\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mmbra\n",
    "import mmbracategories\n",
    "import torch\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "data_dir_root = os.path.join('./data', 'ThingsEEG-Text')\n",
    "\n",
    "sbj_list = ['sub-01', 'sub-02', 'sub-03']\n",
    "\n",
    "image_model = 'pytorch/cornet_s'\n",
    "text_model = 'CLIPText'\n",
    "roi = '17channels'\n",
    "\n",
    "for sbj in sbj_list:\n",
    "    brain_dir = os.path.join(data_dir_root, 'brain_feature', roi, sbj)\n",
    "    image_dir_seen = os.path.join(data_dir_root, 'visual_feature/ThingsTrain', image_model, sbj)\n",
    "    image_dir_unseen = os.path.join(data_dir_root, 'visual_feature/ThingsTest', image_model, sbj)\n",
    "    text_dir_seen = os.path.join(data_dir_root, 'textual_feature/ThingsTrain/text', text_model, sbj)\n",
    "    text_dir_unseen = os.path.join(data_dir_root, 'textual_feature/ThingsTest/text', text_model, sbj)\n",
    "\n",
    "    # ---- seen ----\n",
    "    brain_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['data'].astype('double') * 2.0\n",
    "    brain_seen = brain_seen[:, :, 27:60]\n",
    "    brain_seen = np.reshape(brain_seen, (brain_seen.shape[0], -1))\n",
    "\n",
    "    image_seen = sio.loadmat(os.path.join(image_dir_seen, 'feat_pca_train.mat'))['data'].astype('double') * 50.0\n",
    "    image_seen = image_seen[:, 0:100]\n",
    "\n",
    "    text_seen = sio.loadmat(os.path.join(text_dir_seen, 'text_feat_train.mat'))['data'].astype('double') * 2.0\n",
    "\n",
    "    label_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['class_idx'].T.astype('int')\n",
    "\n",
    "    # ---- unseen ----\n",
    "    brain_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['data'].astype('double') * 2.0\n",
    "    brain_unseen = brain_unseen[:, :, 27:60]\n",
    "    brain_unseen = np.reshape(brain_unseen, (brain_unseen.shape[0], -1))\n",
    "\n",
    "    image_unseen = sio.loadmat(os.path.join(image_dir_unseen, 'feat_pca_test.mat'))['data'].astype('double') * 50.0\n",
    "    image_unseen = image_unseen[:, 0:100]\n",
    "\n",
    "    text_unseen = sio.loadmat(os.path.join(text_dir_unseen, 'text_feat_test.mat'))['data'].astype('double') * 2.0\n",
    "\n",
    "    label_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['class_idx'].T.astype('int')\n",
    "\n",
    "    # collect\n",
    "    all_brain_seen.append(brain_seen)\n",
    "    all_image_seen.append(image_seen)\n",
    "    all_text_seen.append(text_seen)\n",
    "    all_label_seen.append(label_seen)\n",
    "\n",
    "    all_brain_unseen.append(brain_unseen)\n",
    "    all_image_unseen.append(image_unseen)\n",
    "    all_text_unseen.append(text_unseen)\n",
    "    all_label_unseen.append(label_unseen)\n",
    "\n",
    "# stack across subjects\n",
    "brain_seen  = torch.from_numpy(np.vstack(all_brain_seen))\n",
    "image_seen  = torch.from_numpy(np.vstack(all_image_seen))\n",
    "text_seen   = torch.from_numpy(np.vstack(all_text_seen))\n",
    "label_seen  = torch.from_numpy(np.vstack(all_label_seen))\n",
    "\n",
    "brain_unseen = torch.from_numpy(np.vstack(all_brain_unseen))\n",
    "image_unseen = torch.from_numpy(np.vstack(all_image_unseen))\n",
    "text_unseen  = torch.from_numpy(np.vstack(all_text_unseen))\n",
    "label_unseen = torch.from_numpy(np.vstack(all_label_unseen))\n",
    "\n",
    "print('seen_brain_samples=', brain_seen.shape[0], ', seen_brain_features=', brain_seen.shape[1])\n",
    "print('seen_image_samples=', image_seen.shape[0], ', seen_image_features=', image_seen.shape[1])\n",
    "print('seen_text_samples=', text_seen.shape[0], ', seen_text_features=', text_seen.shape[1])\n",
    "print('seen_label=', label_seen.shape)\n",
    "\n",
    "print('unseen_brain_samples=', brain_unseen.shape[0], ', unseen_brain_features=', brain_unseen.shape[1])\n",
    "print('unseen_image_samples=', image_unseen.shape[0], ', unseen_image_features=', image_unseen.shape[1])\n",
    "print('unseen_text_samples=', text_unseen.shape[0], ', unseen_text_features=', text_unseen.shape[1])\n",
    "print('unseen_label=', label_unseen.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db215f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\nTo ensure a strict zero-shot learning setup, we perform a \\nclass-level split of the label space into disjoint seen and unseen sets. \\nAll model development, including embedding refinement, \\nuses only samples from seen classes, while evaluation is performed exclusively \\non unseen classes. This prevents information leakage and aligns with the formal \\ndefinition of zero-shot learning.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''   \n",
    "To ensure a strict zero-shot learning setup, we perform a \n",
    "class-level split of the label space into disjoint seen and unseen sets. \n",
    "All model development, including embedding refinement, \n",
    "uses only samples from seen classes, while evaluation is performed exclusively \n",
    "on unseen classes. This prevents information leakage and aligns with the formal \n",
    "definition of zero-shot learning.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611a0c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled samples: 97620\n",
      "Pooled classes: 1654\n",
      "Seen classes: 1323\n",
      "Unseen classes: 331\n",
      "Final zero-shot split:\n",
      "Seen samples: 76170\n",
      "Unseen samples: 21450\n",
      "Class overlap: 0\n"
     ]
    }
   ],
   "source": [
    "#to avoid data leakage, we must split by classes (80% seen, 20% unseen)\n",
    "# ============================\n",
    "# ZERO-SHOT CLASS-LEVEL SPLIT\n",
    "# ============================\n",
    "\n",
    "# flatten labels to 1D numpy arrays\n",
    "\n",
    "'''\n",
    "\n",
    "y_seen_all = label_seen.numpy().reshape(-1)\n",
    "y_unseen_all = label_unseen.numpy().reshape(-1)\n",
    "\n",
    "all_classes = np.unique(np.concatenate([y_seen_all, y_unseen_all]))\n",
    "\n",
    "print(f\"Total number of classes: {len(all_classes)}\")\n",
    "\n",
    "#reproducible split\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(all_classes)\n",
    "\n",
    "n_seen_classes = int(0.8 * len(all_classes))\n",
    "seen_classes = all_classes[:n_seen_classes]\n",
    "unseen_classes = all_classes[n_seen_classes:]\n",
    "\n",
    "print(f\"Seen classes: {len(seen_classes)}\")\n",
    "print(f\"Unseen classes: {len(unseen_classes)}\")\n",
    "\n",
    "#create masks, ensure no leakage\n",
    "\n",
    "seen_mask = np.isin(y_seen_all, seen_classes)\n",
    "unseen_mask = np.isin(y_unseen_all, unseen_classes)\n",
    "\n",
    "#final zero-shot datasets\n",
    "\n",
    "X_seen = brain_seen[seen_mask]\n",
    "y_seen = label_seen[seen_mask]\n",
    "\n",
    "X_unseen = brain_unseen[unseen_mask]\n",
    "y_unseen = label_unseen[unseen_mask]\n",
    "\n",
    "print(\"Final zero-shot split:\")\n",
    "print(\"Seen samples:\", X_seen.shape[0])\n",
    "print(\"Unseen samples:\", X_unseen.shape[0])\n",
    "\n",
    "\n",
    "# masks over each pool\n",
    "seen_in_seenpool   = np.isin(y_seen_all, seen_classes)\n",
    "unseen_in_seenpool = np.isin(y_seen_all, unseen_classes)\n",
    "\n",
    "seen_in_unseenpool   = np.isin(y_unseen_all, seen_classes)\n",
    "unseen_in_unseenpool = np.isin(y_unseen_all, unseen_classes)\n",
    "\n",
    "print(\"Seen-class samples in seen pool:\",   seen_in_seenpool.sum())\n",
    "print(\"Unseen-class samples in seen pool:\", unseen_in_seenpool.sum())\n",
    "print(\"Seen-class samples in unseen pool:\",   seen_in_unseenpool.sum())\n",
    "print(\"Unseen-class samples in unseen pool:\", unseen_in_unseenpool.sum())\n",
    "''' \n",
    "\n",
    "X_all = torch.cat([brain_seen, brain_unseen], dim=0)\n",
    "y_all = torch.cat([label_seen, label_unseen], dim=0).numpy().reshape(-1).astype(int)\n",
    "\n",
    "T_all = torch.cat([text_seen, text_unseen], dim=0)\n",
    "\n",
    "print(\"Pooled samples:\", X_all.shape[0])\n",
    "print(\"Pooled classes:\", len(np.unique(y_all)))\n",
    "\n",
    "#split label space (class-level ZSL)\n",
    "\n",
    "all_classes = np.unique(y_all)\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(all_classes)\n",
    "\n",
    "n_seen_classes = int(0.8 * len(all_classes))\n",
    "seen_classes = all_classes[:n_seen_classes]\n",
    "unseen_classes = all_classes[n_seen_classes:]\n",
    "\n",
    "print(f\"Seen classes: {len(seen_classes)}\")\n",
    "print(f\"Unseen classes: {len(unseen_classes)}\")\n",
    "\n",
    "seen_mask = np.isin(y_all, seen_classes)\n",
    "unseen_mask = np.isin(y_all, unseen_classes)\n",
    "\n",
    "X_seen = X_all[seen_mask]\n",
    "X_unseen = X_all[unseen_mask]\n",
    "\n",
    "y_seen = torch.from_numpy(y_all[seen_mask]).view(-1,1)\n",
    "y_unseen = torch.from_numpy(y_all[unseen_mask]).view(-1,1)\n",
    "\n",
    "T_seen = T_all[seen_mask]\n",
    "T_unseen = T_all[unseen_mask]\n",
    "\n",
    "print(\"Final zero-shot split:\")\n",
    "print(\"Seen samples:\", X_seen.shape[0])\n",
    "print(\"Unseen samples:\", X_unseen.shape[0])\n",
    "\n",
    "seen_set = set(np.unique(y_seen.numpy().reshape(-1)))\n",
    "unseen_set = set(np.unique(y_unseen.numpy().reshape(-1)))\n",
    "\n",
    "print(\"Class overlap:\", len(seen_set.intersection(unseen_set)))\n",
    "assert len(seen_set.intersection(unseen_set)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f7ba2",
   "metadata": {},
   "source": [
    "## Creating baseline zero-shot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3880773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG test shape: (21450, 512)\n",
      "Alignment OK: 21450\n"
     ]
    }
   ],
   "source": [
    "#EEG features (unseen only, as baseline does not train)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "X_seenNP = X_seen.numpy()\n",
    "X_unseenNP = X_unseen.numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_seen_scaled = scaler.fit_transform(X_seenNP)\n",
    "X_unseen_scaled = scaler.transform(X_unseenNP)\n",
    "\n",
    "#project EEG to 512 dims to match text (fit on seen only)\n",
    "pca = PCA(n_components=512, random_state=0)\n",
    "X_seen_512 = pca.fit_transform(X_seen_scaled)\n",
    "X_test = pca.transform(X_unseen_scaled)\n",
    "\n",
    "print(\"EEG test shape:\", X_test.shape)\n",
    "\n",
    "assert X_unseen.shape[0] == y_unseen.shape[0]\n",
    "assert T_unseen.shape[0] == y_unseen.shape[0]\n",
    "assert X_test.shape[0] == y_unseen.shape[0]\n",
    "print(\"Alignment OK:\", X_unseen.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e185d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen classes (in eval set): 331\n"
     ]
    }
   ],
   "source": [
    "#flatten labels\n",
    "y_unseenNP = y_unseen.numpy().reshape(-1).astype(int)\n",
    "\n",
    "unseen_classes = np.unique(y_unseenNP)\n",
    "print('Unseen classes (in eval set):', len(unseen_classes))\n",
    "\n",
    "#semantic prototypes is a dict {class_id : vector}\n",
    "#mean text embedding per unseen class for the prototype vector for each class\n",
    "text_unseen_np = T_unseen.numpy()\n",
    "\n",
    "\n",
    "semantic_proto = {}\n",
    "for i in unseen_classes:\n",
    "    idx = np.where(y_unseenNP == i)[0]\n",
    "    semantic_proto[i] = text_unseen_np[idx].mean(axis=0) #we represent each class by the centroid of its semantic embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6cadded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity inference\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#stack prototypes into matrix\n",
    "proto_label = list(semantic_proto.keys())\n",
    "proto_matrix = np.stack([semantic_proto[i] for i in proto_label])\n",
    "\n",
    "#normalise for cosine\n",
    "Xn = X_test / (np.linalg.norm(X_test, axis=1, keepdims=True)+1e-8)\n",
    "Pn = proto_matrix / (np.linalg.norm(proto_matrix, axis=1, keepdims=True)+1e-8)\n",
    "\n",
    "#cosine similarities: (N_unseen, C_unseen)\n",
    "\n",
    "S = Xn @ Pn.T\n",
    "\n",
    "y_pred = np.array([proto_label[j] for j in np.argmax(S, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b4891ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot baseline accuracy: 0.0049\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_unseenNP, y_pred)\n",
    "print(f\"Zero-shot baseline accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebb8728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1: 0.0048951048951048955\n",
      "Top-5 0.021212121212121213\n"
     ]
    }
   ],
   "source": [
    "def topk_accuracy(S, true_labels, class_labels, k=5):\n",
    "    class_labels = np.array(class_labels)\n",
    "    # indices of top-k predictions per sample (highest scores)\n",
    "    topk_idx = np.argpartition(S, -k, axis=1)[:, -k:]\n",
    "    topk_labels = class_labels[topk_idx]\n",
    "    true_labels = np.array(true_labels).astype(class_labels.dtype)\n",
    "\n",
    "    # vectorised membership test\n",
    "    return np.mean((topk_labels == true_labels[:, None]).any(axis=1))\n",
    "\n",
    "\n",
    "print(\"Top-1:\", topk_accuracy(S, y_unseenNP, proto_label, k=1))\n",
    "print(\"Top-5\", topk_accuracy(S, y_unseenNP, proto_label, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5f55b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen classes: 331\n",
      "Unseen samples: 21450\n",
      "Min samples per unseen class: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Unseen classes:\", len(np.unique(y_unseenNP)))\n",
    "print(\"Unseen samples:\", len(y_unseenNP))\n",
    "print(\"Min samples per unseen class:\", min([(y_unseenNP==c).sum() for c in np.unique(y_unseenNP)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669c32b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen classes: 331\n",
      "Chance Top-1: 0.0030211480362537764\n",
      "Chance Top-5: 0.015105740181268883\n"
     ]
    }
   ],
   "source": [
    "C = len(np.unique(y_unseenNP))\n",
    "print(\"Unseen classes:\", C)\n",
    "print(\"Chance Top-1:\", 1.0/C)\n",
    "print(\"Chance Top-5:\", 5.0/C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955242f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "The zero-shot baseline achieves performance marginally \n",
    "above chance for both Top-1 and Top-5 accuracy, indicating \n",
    "weak but non-random alignment between EEG representations \n",
    "and semantic class embeddings. This motivates the use of \n",
    "embedding refinement to improve cross-modal alignment.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
